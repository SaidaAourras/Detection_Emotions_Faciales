{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bad7000b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: C:\\Users\\hp\\.cache\\kagglehub\\datasets\\ananthu017\\emotion-detection-fer\\versions\\1\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "path = kagglehub.dataset_download(\"ananthu017/emotion-detection-fer\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bc931a31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---test\n",
      "['angry', 'disgusted', 'fearful', 'happy', 'neutral', 'sad', 'surprised']\n",
      "---train\n",
      "['angry', 'disgusted', 'fearful', 'happy', 'neutral', 'sad', 'surprised']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# print(os.listdir(path))\n",
    "\n",
    "for item in os.listdir(path):\n",
    "  item_path = os.path.join(path, item)\n",
    "  if os.path.isdir(item_path):\n",
    "    print(f'---{item}')\n",
    "    print(os.listdir(item_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dfc21a7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28709 files belonging to 7 classes.\n",
      "Found 7178 files belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "train_path = os.path.join(path, 'train')\n",
    "test_path = os.path.join(path, 'test')\n",
    "\n",
    "train_data = tf.keras.utils.image_dataset_from_directory(train_path , batch_size=1000 ,image_size=(48,48) , label_mode='categorical')\n",
    "test_data = tf.keras.utils.image_dataset_from_directory(test_path , batch_size=1000,image_size=(48,48) , label_mode='categorical')\n",
    "\n",
    "# print(type(train_data)) # tensorflow.python.data.ops.prefetch_op._PrefetchDataset\n",
    "\n",
    "# for i in train_data:\n",
    "#   print(type(i)) # tuple\n",
    "#   print(i[1])\n",
    "#   break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d4997c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['angry', 'disgusted', 'fearful', 'happy', 'neutral', 'sad', 'surprised']\n",
      "(1000, 48, 48, 3)\n",
      "(1000, 7)\n",
      "tf.Tensor([1. 0. 0. 0. 0. 0. 0.], shape=(7,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(train_data.class_names)\n",
    "classe = train_data.class_names\n",
    "for images , labels in train_data:\n",
    "  print(images.shape)\n",
    "  print(labels.shape)\n",
    "  print(labels[1])\n",
    "  break\n",
    "\n",
    "# print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3a5b0620",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "only integer scalar arrays can be converted to a scalar index",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[47]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     12\u001b[39m     plt.subplot(\u001b[32m3\u001b[39m,\u001b[32m3\u001b[39m,i)\n\u001b[32m     13\u001b[39m     plt.imshow(image)\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     plt.title(\u001b[43mclasse\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[32m     15\u001b[39m     plt.axis(\u001b[33m\"\u001b[39m\u001b[33moff\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     17\u001b[39m plt.tight_layout()\n",
      "\u001b[31mTypeError\u001b[39m: only integer scalar arrays can be converted to a scalar index"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(4, 4))\n",
    "\n",
    "for images , labels in train_data.take(2):\n",
    "  for i in range(1,10):\n",
    "    # print(images.shape)\n",
    "    # print(np.unique(labels))\n",
    "    image = images[i].numpy().astype('uint8')\n",
    "    label = labels[i].numpy().astype('uint8')\n",
    "    # print(image)\n",
    "    plt.subplot(3,3,i)\n",
    "    plt.imshow(image)\n",
    "    plt.title(classe[label])\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c94874",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import tensorflow as tf\n",
    "\n",
    "for images, labels in train_data.take(1):\n",
    "    for i in range(5):\n",
    "        img = images[i].numpy().astype('uint8')\n",
    "        img_bgr = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "        cv2.imshow(classe[labels[i]], img_bgr)\n",
    "        cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "783604a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hp\\Desktop\\briefs\\Detection_Emotions_Faciales\\venv\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_4\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_4\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">46</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">46</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">262,272</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">903</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_12 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m46\u001b[0m, \u001b[38;5;34m46\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_12 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m23\u001b[0m, \u001b[38;5;34m23\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_13 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m21\u001b[0m, \u001b[38;5;34m21\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_13 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_14 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │        \u001b[38;5;34m73,856\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_14 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_4 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_8 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m262,272\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_9 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m)              │           \u001b[38;5;34m903\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">356,423</span> (1.36 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m356,423\u001b[0m (1.36 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">356,423</span> (1.36 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m356,423\u001b[0m (1.36 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create a model cnn with flattn & dense\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.Conv2D(32, (3,3) , activation='relu', input_shape=(48,48,3)),\n",
    "    layers.MaxPool2D(pool_size=(2,2)),\n",
    "    \n",
    "    layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    layers.MaxPool2D((2,2)),\n",
    "\n",
    "    layers.Conv2D(128, (3,3), activation='relu'),\n",
    "    layers.MaxPool2D((2,2)),\n",
    "    \n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128 , activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(7 , activation='softmax')\n",
    "])\n",
    "# optimiser\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c696236e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 3s/step - accuracy: 0.1844 - loss: 7.4412\n",
      "Epoch 2/50\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 2s/step - accuracy: 0.2107 - loss: 1.8791\n",
      "Epoch 3/50\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 2s/step - accuracy: 0.2309 - loss: 1.8494\n",
      "Epoch 4/50\n",
      "\u001b[1m17/29\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m1:07\u001b[0m 6s/step - accuracy: 0.2459 - loss: 1.8229"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[49]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\Desktop\\briefs\\Detection_Emotions_Faciales\\venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\Desktop\\briefs\\Detection_Emotions_Faciales\\venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:399\u001b[39m, in \u001b[36mTensorFlowTrainer.fit\u001b[39m\u001b[34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[39m\n\u001b[32m    397\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m begin_step, end_step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[32m    398\u001b[39m     callbacks.on_train_batch_begin(begin_step)\n\u001b[32m--> \u001b[39m\u001b[32m399\u001b[39m     logs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    400\u001b[39m     callbacks.on_train_batch_end(end_step, logs)\n\u001b[32m    401\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stop_training:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\Desktop\\briefs\\Detection_Emotions_Faciales\\venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:241\u001b[39m, in \u001b[36mTensorFlowTrainer._make_function.<locals>.function\u001b[39m\u001b[34m(iterator)\u001b[39m\n\u001b[32m    237\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfunction\u001b[39m(iterator):\n\u001b[32m    238\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[32m    239\u001b[39m         iterator, (tf.data.Iterator, tf.distribute.DistributedIterator)\n\u001b[32m    240\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m241\u001b[39m         opt_outputs = \u001b[43mmulti_step_on_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    242\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs.has_value():\n\u001b[32m    243\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\Desktop\\briefs\\Detection_Emotions_Faciales\\venv\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\Desktop\\briefs\\Detection_Emotions_Faciales\\venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[39m, in \u001b[36mFunction.__call__\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    830\u001b[39m compiler = \u001b[33m\"\u001b[39m\u001b[33mxla\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mnonXla\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m._jit_compile):\n\u001b[32m--> \u001b[39m\u001b[32m833\u001b[39m   result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    835\u001b[39m new_tracing_count = \u001b[38;5;28mself\u001b[39m.experimental_get_tracing_count()\n\u001b[32m    836\u001b[39m without_tracing = (tracing_count == new_tracing_count)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\Desktop\\briefs\\Detection_Emotions_Faciales\\venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:878\u001b[39m, in \u001b[36mFunction._call\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    875\u001b[39m \u001b[38;5;28mself\u001b[39m._lock.release()\n\u001b[32m    876\u001b[39m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[32m    877\u001b[39m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m878\u001b[39m results = \u001b[43mtracing_compilation\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[32m    880\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    881\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._created_variables:\n\u001b[32m    882\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCreating variables on a non-first call to a function\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    883\u001b[39m                    \u001b[33m\"\u001b[39m\u001b[33m decorated with tf.function.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\Desktop\\briefs\\Detection_Emotions_Faciales\\venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[39m, in \u001b[36mcall_function\u001b[39m\u001b[34m(args, kwargs, tracing_options)\u001b[39m\n\u001b[32m    137\u001b[39m bound_args = function.function_type.bind(*args, **kwargs)\n\u001b[32m    138\u001b[39m flat_inputs = function.function_type.unpack_inputs(bound_args)\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[32m    140\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\Desktop\\briefs\\Detection_Emotions_Faciales\\venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[39m, in \u001b[36mConcreteFunction._call_flat\u001b[39m\u001b[34m(self, tensor_inputs, captured_inputs)\u001b[39m\n\u001b[32m   1318\u001b[39m possible_gradient_type = gradients_util.PossibleTapeGradientTypes(args)\n\u001b[32m   1319\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type == gradients_util.POSSIBLE_GRADIENT_TYPES_NONE\n\u001b[32m   1320\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[32m   1321\u001b[39m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_inference_function\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1323\u001b[39m forward_backward = \u001b[38;5;28mself\u001b[39m._select_forward_and_backward_functions(\n\u001b[32m   1324\u001b[39m     args,\n\u001b[32m   1325\u001b[39m     possible_gradient_type,\n\u001b[32m   1326\u001b[39m     executing_eagerly)\n\u001b[32m   1327\u001b[39m forward_function, args_with_tangents = forward_backward.forward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\Desktop\\briefs\\Detection_Emotions_Faciales\\venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[39m, in \u001b[36mAtomicFunction.call_preflattened\u001b[39m\u001b[34m(self, args)\u001b[39m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core.Tensor]) -> Any:\n\u001b[32m    215\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m   flat_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.function_type.pack_output(flat_outputs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\Desktop\\briefs\\Detection_Emotions_Faciales\\venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[39m, in \u001b[36mAtomicFunction.call_flat\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m record.stop_recording():\n\u001b[32m    250\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._bound_context.executing_eagerly():\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_bound_context\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction_type\u001b[49m\u001b[43m.\u001b[49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    256\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    257\u001b[39m     outputs = make_call_op_in_graph(\n\u001b[32m    258\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    259\u001b[39m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[32m    260\u001b[39m         \u001b[38;5;28mself\u001b[39m._bound_context.function_call_options.as_attrs(),\n\u001b[32m    261\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\Desktop\\briefs\\Detection_Emotions_Faciales\\venv\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1688\u001b[39m, in \u001b[36mContext.call_function\u001b[39m\u001b[34m(self, name, tensor_inputs, num_outputs)\u001b[39m\n\u001b[32m   1686\u001b[39m cancellation_context = cancellation.context()\n\u001b[32m   1687\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1688\u001b[39m   outputs = \u001b[43mexecute\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1689\u001b[39m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1690\u001b[39m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1691\u001b[39m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1692\u001b[39m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1693\u001b[39m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1694\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1695\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1696\u001b[39m   outputs = execute.execute_with_cancellation(\n\u001b[32m   1697\u001b[39m       name.decode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m   1698\u001b[39m       num_outputs=num_outputs,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1702\u001b[39m       cancellation_manager=cancellation_context,\n\u001b[32m   1703\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\Desktop\\briefs\\Detection_Emotions_Faciales\\venv\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[39m, in \u001b[36mquick_execute\u001b[39m\u001b[34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     52\u001b[39m   ctx.ensure_initialized()\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m   tensors = \u001b[43mpywrap_tfe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     56\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "model.fit(train_data , epochs=50 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5827a67a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 737ms/step - accuracy: 0.4407 - loss: 1.4798\n",
      "[1.4798308610916138, 0.4406519830226898]\n"
     ]
    }
   ],
   "source": [
    "print(model.evaluate(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e743b8f",
   "metadata": {},
   "source": [
    "## save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f506c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('CNN_model.keras', include_optimizer=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
